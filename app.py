import streamlit as st
import os
#from dotenv import load_dotenv
from mistralai import Mistral, UserMessage

# Load environment variables
#load_dotenv()

# Set page configuration
st.set_page_config(page_title="ë¦¬ì„œì¹˜ë´‡", page_icon="ğŸ¤–")

# Mistral í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” 
# API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë‚˜ Streamlit secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°
try:
    client = Mistral(api_key=os.getenv("MISTRAL_API_KEY") or st.secrets["MISTRAL_API_KEY"])
except Exception as e:
    st.error(f"API í‚¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    client = None

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
DEFAULT_MODEL = "mistral-large-latest"

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜
SYSTEM_MESSAGE = '''
You are a professional research agent tasked with conducting thorough research on a given topic. Your goal is to provide comprehensive and accurate information to assist in decision-making or further analysis.

[IMPORTANT] Final output should be written in Korean.

###

Please follow these steps to complete your research task:

1. Think deeply about the user's given research topic. Consider its various aspects, potential subtopics, and areas that might require investigation.

2. Generate a list of useful search queries that will help you gather relevant information. These queries should cover different angles of the topic and be designed to yield comprehensive results.

3. Use the `search tool` to conduct your research. Make sure to explore multiple reliable sources and gather a diverse range of information.

4. Compile your research findings, ensuring that you capture key points, relevant data, and any conflicting information you may encounter.

5. Evaluate the credibility of your sources and consider potential biases or limitations in your research.

6. Prepare a summary of your research results, highlighting the most important discoveries and their implications.

7. Once your research is complete, transfer the results to the `Supervisor Agent` for review.

Throughout this process, wrap your thought process inside <thinking> tags. This will help ensure a thorough and well-considered approach to the research task. In your thinking, include:

- Your interpretation of the research topic
- Rationale for chosen search queries
- Evaluation of source credibility
- Consideration of potential biases or limitations
- Reasoning behind key findings and conclusions

Your final output should be structured as follows:

<research_report>
<search_queries>
[List the main search queries you used]
</search_queries>

<key_findings>
[Bullet points of the most important information discovered]
</key_findings>

<summary>
[A concise summary of your research results, including any major conclusions or implications]
</summary>
</research_report>

Remember to be objective, thorough, and critical in your research approach. If you encounter any limitations or areas where further research is needed, please mention these in your summary.
'''

# Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def initialize_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "system", 
             "content": SYSTEM_MESSAGE}
        ]
    if "mistral_model" not in st.session_state:
        st.session_state["mistral_model"] = DEFAULT_MODEL

# ë©”ì‹œì§€ í‘œì‹œ í•¨ìˆ˜
def display_messages():
    for message in st.session_state.messages[1:]:  # system message ì œì™¸
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# ì±„íŒ… ì‘ë‹µ ì²˜ë¦¬ í•¨ìˆ˜
def get_chat_response(messages):
    try:
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë©”ì‹œì§€ ì¤€ë¹„
        chat_response = client.chat.stream(
            model=st.session_state["mistral_model"],
            messages=messages
        )
#        print(dir(chat_response))
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        response_content = ""
        for chunk in chat_response:
#            print(chunk)
            if chunk.data:
                response_content += chunk.data.choices[0].delta.content  # ì—¬ê¸° ìˆ˜ì •
#                print(response_content)
#                print(f"Received chunk: {chunk.data.choices[0].delta.content}")
        return response_content
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•´. í˜„ì¬ ëŒ€í™”ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ë¬¸ì œê°€ ìˆì–´. ë‹¤ì‹œ ì‹œë„í•´ì¤„ë˜? "
# ë©”ì¸ ì•± ë¡œì§
def main():
    # í˜ì´ì§€ ì œëª©
    st.title("ğŸ¤– ì „ë¬¸ë¦¬ì„œì¹˜ë´‡")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()

    # ê¸°ì¡´ ë©”ì‹œì§€ í‘œì‹œ
    display_messages()

    # API í´ë¼ì´ì–¸íŠ¸ í™•ì¸
    if not client:
        st.warning("API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ì–´. API í‚¤ë¥¼ í™•ì¸í•´ì¤˜! ğŸš¨")
        return

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë¬´ìŠ¨ ì–˜ê¸° í•˜ê³  ì‹¶ì–´?"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        user_message = {"role": "user", "content": prompt}
        st.session_state.messages.append(user_message)
        
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant"):
            with st.spinner("ìƒê° ì¤‘..."):
                # ì „ì²´ ë©”ì‹œì§€ ì¤€ë¹„ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨)
                response_messages = st.session_state.messages

                # ì‘ë‹µ ìƒì„±
                response = get_chat_response(response_messages)

                # ì‘ë‹µ í‘œì‹œ ë° ì„¸ì…˜ì— ì¶”ê°€
                st.markdown(response)
                
                # AI ë©”ì‹œì§€ ì„¸ì…˜ì— ì¶”ê°€
                ai_message = {"role": "assistant", "content": response}
                st.session_state.messages.append(ai_message)

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    main()